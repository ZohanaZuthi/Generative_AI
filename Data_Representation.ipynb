{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction is the process of transforming raw data into a set of meaningful attributes or features that can be used for machine learning, data analysis, or pattern recognition. The goal is to extract the most relevant information from the raw data while reducing its complexity, making it easier for algorithms to analyze and process.\n",
    "\n",
    "In machine learning, raw data (such as images, text, or time-series data) may contain many variables or characteristics, but not all of them are useful for prediction or analysis. Feature extraction helps to identify and isolate the most important aspects of the data that contribute to the task at hand (e.g., classification, regression, clustering).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For text to numerical format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply two kinds of technique:\n",
    "1. One Hot Encoding\n",
    "2. Bag of Words\n",
    "\n",
    "One Hot encoding increases the complexity and dimension\n",
    "so it is wiser to apply bag of words . Especially in sentiments analysis as it is based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"text\":[\"people watch dswithbappy\",\n",
    "\"dswithbappy watch dswithbappy\",\n",
    "\"people write comment\",\n",
    "\"dswithbappy write comment\"],\"output\":[1,1,0,0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch dswithbappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dswithbappy watch dswithbappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dswithbappy write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text  output\n",
       "0       people watch dswithbappy       1\n",
       "1  dswithbappy watch dswithbappy       1\n",
       "2           people write comment       0\n",
       "3      dswithbappy write comment       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'dswithbappy': 1, 'write': 4, 'comment': 0}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0],\n",
       "       [0, 2, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), **n-grams** refer to contiguous sequences of `n` items (usually words) in a text. **Unigrams**, **bigrams**, and **trigrams** are specific types of n-grams where `n` is 1, 2, and 3, respectively. These are used as features in a **Bag of Words** (BoW) model to represent the text in a structured form.\n",
    "\n",
    "### 1. **Unigrams (1-grams)**\n",
    "Unigrams are individual words from the text. For example, the sentence:\n",
    "- \"I love coding\"\n",
    "\n",
    "Would be broken down into the following unigrams:\n",
    "- **[\"I\", \"love\", \"coding\"]**\n",
    "\n",
    "### 2. **Bigrams (2-grams)**\n",
    "Bigrams are pairs of consecutive words in a text. For example, the same sentence:\n",
    "- \"I love coding\"\n",
    "\n",
    "Would be broken down into the following bigrams:\n",
    "- **[\"I love\", \"love coding\"]**\n",
    "\n",
    "### 3. **Trigrams (3-grams)**\n",
    "Trigrams are triples of consecutive words. For the sentence:\n",
    "- \"I love coding\"\n",
    "\n",
    "The trigrams would be:\n",
    "- **[\"I love coding\"]**\n",
    "\n",
    "### Bag of Words (BoW) Representation\n",
    "The **Bag of Words** model treats a text as an unordered collection of words or n-grams (unigrams, bigrams, trigrams, etc.), without considering the sequence in which they appear. In a BoW approach, you count how many times each n-gram occurs in the text.\n",
    "\n",
    "For example:\n",
    "- **Text 1**: \"I love coding\"\n",
    "- **Text 2**: \"I love machine learning\"\n",
    "\n",
    "The unigrams, bigrams, and trigrams would be extracted as follows:\n",
    "\n",
    "- **Unigrams**: [\"I\", \"love\", \"coding\", \"machine\", \"learning\"]\n",
    "- **Bigrams**: [\"I love\", \"love coding\", \"coding machine\", \"machine learning\"]\n",
    "- **Trigrams**: [\"I love coding\", \"love coding machine\", \"coding machine learning\"]\n",
    "\n",
    "Then, you can represent the frequency of each n-gram in a matrix or vector format.\n",
    "\n",
    "### Usage in Machine Learning:\n",
    "N-grams (unigrams, bigrams, trigrams) are widely used in feature extraction for text classification tasks, such as sentiment analysis, spam detection, etc. By representing text as vectors of n-gram frequencies, machine learning algorithms can process the textual data in a more structured form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 4, 'coding': 0, 'love coding': 5, 'machine': 7, 'learning': 3, 'love machine': 6, 'machine learning': 8, 'enjoy': 1, 'solving': 11, 'problems': 9, 'with': 13, 'enjoy solving': 2, 'solving problems': 12, 'problems with': 10, 'with coding': 14}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus (list of sentences)\n",
    "corpus = [\n",
    "    \"I love coding\",\n",
    "    \"I love machine learning\",\n",
    "    \"I enjoy solving problems with coding\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance for unigrams, bigrams, and trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the corpus to get n-grams\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used in text processing to evaluate the importance of a word (or term) within a document relative to a corpus (a collection of documents). It combines two components: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "1. Term Frequency (TF)\n",
    "This measures how frequently a term appears in a document.\n",
    "2. Inverse Document Frequency (IDF)\n",
    "This measures the importance of the term across the entire corpus. If a term appears in many documents, it is considered less important, and vice versa.\n",
    "3. TF-IDF Score\n",
    "The TF-IDF score is simply the product of TF and IDF.\n",
    "1. Term Frequency (TF):\n",
    "\n",
    "   TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)\n",
    "\n",
    "2. Inverse Document Frequency (IDF):\n",
    "\n",
    "   IDF(t) = log ( (Total number of documents) / (Number of documents containing the term t) )\n",
    "\n",
    "3. TF-IDF Score:\n",
    "\n",
    "   TF-IDF(t, d) = TF(t, d) * IDF(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
